# Pre-requisite
    1.  Install Ollama model first and then start the service
        Command to start the service: ollama run llama3.2

# Download the project from the GitHub and configure locally on your machine 
    1.  Run all the cells sequentially from capstone_project.ipynyb



